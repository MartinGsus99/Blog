# 深度学习

机器学习流程：

- 数据获取
- 特征工程
- 建立模型
- 评估应用

> 深度学习解决了机器学习人工参与大的问题；

特征工程的作用：

- 数据特征决定了模型的上限；
- 预处理和特征提取是最核心的；
- 算法与参数选择决定了如何逼近这个上限；

特征如何提取：

- 传统特征提取
- 深度学习：黑盒子，拿到数据神经网络能够自动的处理数据，提取神经网络自己能认识的特征，选择最合适的特征；

深度学习应用：

- 图像识别；
- 人脸识别；
- 医学图像识别、癌细胞检测；
- 换脸；
- 上色；
- 分辨率重构；

深度学习缺点：

- 计算量太大；
- 参数数量大；

计算机视觉：

- 三维数组表示一张图片；
- 挑战：照射角度、形状改变、遮蔽、背景混入；

机器学习常规套路：

- 收集数据给定标签；
- 训练一个分类器；
- 测试评估；

##### K临近算法

![](E:\Project\Blog\source\_posts\images\QQ20241008-101203.png)

1. 计算一直类别数据集中的点与当前点的距离；
2. 按距离排序；
3. 选择距离最近的K个点；
4. 选择出现频率最高的标签赋予；



#### 数据集

- CIFAR-10：50000个训练数据、10000个测试数据、32*32、10类标签；





#### 距离选择

- L1 Distance

![](E:\Project\Blog\source\_posts\images\QQ20241008-101701.png)

## 一、神经网络基础

### 线性函数（得分函数）

![](E:\Project\Blog\source\_posts\images\QQ20241008-102042.png)
$$
y=W（权重参数）x+b(偏置参数)
$$
![](E:\Project\Blog\source\_posts\images\QQ20241008-102631.png)

![](E:\Project\Blog\source\_posts\images\QQ20241008-103157.png)

### 损失函数

> 结果的得分值有着明显的差异，需要明确当前指导莫ing的效果好坏；

![](E:\Project\Blog\source\_posts\images\QQ20241008-103332.png)

- 1表示detal，🔺表示错误容忍度；

如果两个模型的损失函数的值相同，意味着两个模型一样吗？

![](E:\Project\Blog\source\_posts\images\QQ20241008-103841.png)

模型A只考虑局部一个特征，模型B考虑全局；
$$
损失函数=数据损失+正则化惩罚项
$$
![](E:\Project\Blog\source\_posts\images\QQ20241008-104025.png)

### Softmax分类器

> 现在我们得到的是一个输入的得分值，但是给一个概率更好，如何把得分值转换成一个概率值呢？

$$
g(z)=\frac{1}{1+e^{-z}}
$$

![](E:\Project\Blog\source\_posts\images\QQ20241008-104751.png)

### 前向传播

![](E:\Project\Blog\source\_posts\images\QQ20241008-152750.png)

> 计算出一个loss，完成前向传播

### 反向传播 

> 当我们得到一个目标函数，如何求解？
>
> - 直接求解：不一定可解；
> - 常规套路：交给机器一堆数据，告诉他什么样的许欸小方式是对的（目标函数）；
> - 如何优化？

#### 梯度下降

- 目标函数；
- 寻找山谷的最低点；
- 下山分几步？

> 1. 找到当前最合适的方向；
> 2. 走一小步；
> 3. 按照方法与不发更新参数；

![](E:\Project\Blog\source\_posts\images\QQ20241008-153445.png)

![](E:\Project\Blog\source\_posts\images\QQ20241008-154126.png)

> 一步一步获得loss反向沿着红色的箭头反向传递；

- 加法门单元：均等分配；
- MAX门单元：给最大的；
- 乘法门单元：互换；

![](E:\Project\Blog\source\_posts\images\QQ20241008-154829.png)

### 整体架构

![](E:\Project\Blog\source\_posts\images\QQ20241008-155916.png)

![](E:\Project\Blog\source\_posts\images\QQ20241008-160509.png)

- 层次结构：输入层、隐藏层、输出层；
- 隐藏层：上图将原始输入的三个特征转换为四个特征，将人类认识的特征转换为计算机能认识的信息；

![](E:\Project\Blog\source\_posts\images\QQ20241008-161425.png)

- 非线性：各层的计算顺序不能改变；

![](E:\Project\Blog\source\_posts\images\QQ20241008-161646.png)

神经元个数对网络的影响？

> 神经元越多，过拟合越大，训练集效果越好，训练速度越慢；
>
> 一般64、128、256、512；

![](E:\Project\Blog\source\_posts\images\QQ20241008-163205.png)

惩罚力度对结果的影响？

> 惩罚力度小容易过拟合

![](E:\Project\Blog\source\_posts\images\QQ20241008-162941.png)

### 激活函数

> 激活函数用于非线性变换；
>
> 常用的激活函数：
>
> - Sigmoid
> - Relu
> - Tanh

![](E:\Project\Blog\source\_posts\images\QQ20241008-163952.png)

### 数据预处理

> 不同的预处理结果会使得模型的效果发生很大的差异；

![](https://s3.bmp.ovh/imgs/2024/10/18/4959ee299fccdf9f.png)

参数初始化同样重要，通常使用随机策略进行参数初始化；
$$
W=0.01*np.random.randn(D,H)
$$

### 过拟合

> 过拟合是大问题，非常头疼；

![](https://s3.bmp.ovh/imgs/2024/10/18/8d4b58cf0f5b6a1c.png)

每次进行过程中，选择一部分神经元杀死，称为Drop-Out，如图中❎部分，减小过拟合的影响；

## 二、卷积神经网络

### 卷积神经网络基础

#### 应用

- 分类与检索；
- 检测；
- 超分辨率重构；
- 医学任务、OCR；
- 无人驾驶；
- 人脸识别；

#### 卷积神经网络与传统网络区别

![](https://s3.bmp.ovh/imgs/2024/10/18/46ba123bd5dd32d8.png)

#### 网络结构

1. 输入层；

2. 卷积层：提取特征；

   ![](https://s3.bmp.ovh/imgs/2024/10/18/8fe7de11db5fca3b.png)

   图像颜色通道、RGB

   ![](https://s3.bmp.ovh/imgs/2024/10/18/f7967c19f28b8f61.png)

   ✨可以使用多个卷积核，得到不同的特征结果，使得特征提取结果更加丰富；

   多个特征图进行堆叠；

   > 卷积需要做几次？
   >
   > ![](https://s3.bmp.ovh/imgs/2024/10/18/6b88b19a5f32c8ba.png)

   > 参数：
   >
   > - 滑动窗口步长：每次移动的单元格数量；步长小等于细颗粒度的提取特征，得到特征多，计算慢；一般根据实际任务选择步长，3*3 1 常见；
   > - 卷积核尺寸：卷积核的二维大小；卷积核越小，细颗粒度提取；
   > - 边缘填充：zero-padding；
   > - 卷积核个数：需要得到几个特征图，就是用几个卷积核；
   >
   > ![](https://s3.bmp.ovh/imgs/2024/10/18/bffda4f7665d75bd.png)

3. 池化层：压缩特征；

   > 有选择性的压缩特征数量；几次卷积一次池化；
   >
   > - 最大池化；
   >
   > ![](https://s3.bmp.ovh/imgs/2024/10/18/42c95b0f849e73a9.png)

4. 全连接层：输出分类；

#### 感受野

![](https://s3.bmp.ovh/imgs/2024/10/18/62bbc986dff4c2b1.png)

下一层能感受到上一层区域；

最后的数据值是由多少个原始数据得到的‘

### 经典网络

#### Alexnet

![](https://s3.bmp.ovh/imgs/2024/10/18/a61efb486162bd47.png)

存在的文图：

- filter太大；
- 无pad；

#### Vgg

![](https://s3.bmp.ovh/imgs/2024/10/18/df2d0cf7d85501ea.png)

- 卷积核3*3；
- 经过pooling之后，在下一次卷积时，特征翻倍；

#### Resnet

![](https://s3.bmp.ovh/imgs/2024/10/18/e46032414f0263d2.png)

当发现网络中存在导致效果变差的层时，用第二条线路绕过；

#### 性能比较

![](https://s3.bmp.ovh/imgs/2024/10/18/250e926a6f613782.png)

### 三、递归神经网络RNN

![](https://s3.bmp.ovh/imgs/2024/10/18/6d46404e5ddd58dd.png)

传统的神经网络不会考虑数据之间的关系，比如时序；

#### 主要应用

- 自然语言处理；

![](https://s3.bmp.ovh/imgs/2024/10/18/dfdefd2590d734ab.png)

#### 问题

- 记忆能力过强，计算量巨大；但是前面的信息对于最后的结果并不一定重要；

#### LSTM

